{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d879a99d",
   "metadata": {},
   "source": [
    "# stemmeing using PorterStmmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9924520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5b0c305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'goes', 'going', 'gone']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = ['go','goes','going','gone']\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16447ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0cda34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98441fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go : go\n",
      "goes : goe\n",
      "going : go\n",
      "gone : gone\n"
     ]
    }
   ],
   "source": [
    "for w in word:\n",
    "    print(w,':', ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad3b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a5cea71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program : program\n",
      "programs : program\n",
      "programmer : programm\n",
      "programming : program\n",
      "programmers : programm\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(w, ':', ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc714b1",
   "metadata": {},
   "source": [
    "# stemmer Using Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "787a6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a30d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Programmers program with programming languages\"\n",
    "wd = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b8494e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programmers : programm\n",
      "program : program\n",
      "with : with\n",
      "programming : program\n",
      "languages : languag\n"
     ]
    }
   ],
   "source": [
    "for w in wd:\n",
    "    print(w, ':', ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cfacfc",
   "metadata": {},
   "source": [
    "#  Stemmer using Reduce class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abae4ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Programmers', 'program', 'with', 'programming', 'languages']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "\n",
    "sentence = \"Programmers program with programming languages\"\n",
    "wd = word_tokenize(sentence)\n",
    "wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b5758fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programmers  program  with  program  languag\n"
     ]
    }
   ],
   "source": [
    "sen_re = reduce(lambda x,y: x+ \"  \" + ps.stem(y), wd)\n",
    "print(sen_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cae532",
   "metadata": {},
   "source": [
    "# Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ae5d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee8d3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92032c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = 'I want to changes the world if the world changing my carrers to by changed the wrolds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc03da64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'want',\n",
       " 'to',\n",
       " 'changes',\n",
       " 'the',\n",
       " 'world',\n",
       " 'if',\n",
       " 'the',\n",
       " 'world',\n",
       " 'changing',\n",
       " 'my',\n",
       " 'carrers',\n",
       " 'to',\n",
       " 'by',\n",
       " 'changed',\n",
       " 'the',\n",
       " 'wrolds']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word = word_tokenize(sen)\n",
    "new_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f79ecbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I\n",
      "want want\n",
      "to to\n",
      "changes change\n",
      "the the\n",
      "world world\n",
      "if if\n",
      "the the\n",
      "world world\n",
      "changing changing\n",
      "my my\n",
      "carrers carrers\n",
      "to to\n",
      "by by\n",
      "changed changed\n",
      "the the\n",
      "wrolds wrolds\n"
     ]
    }
   ],
   "source": [
    "for w in new_word:\n",
    "    print(w , le.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21f83994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'changing'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.lemmatize('changing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3058ea6a",
   "metadata": {},
   "source": [
    "# tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6a3210",
   "metadata": {},
   "source": [
    "# NLTK(Natural Language Toolkit) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c0eb7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b99f5af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "sentence = 'I am sajib saha. Working at CIT. As an employee machine Learnig. Do you want to come'\n",
    "word_token = word_tokenize(sentence)\n",
    "sen_token = sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b8aac02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'sajib',\n",
       " 'saha',\n",
       " '.',\n",
       " 'Working',\n",
       " 'at',\n",
       " 'CIT',\n",
       " '.',\n",
       " 'As',\n",
       " 'an',\n",
       " 'employee',\n",
       " 'machine',\n",
       " 'Learnig',\n",
       " '.',\n",
       " 'Do',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'come']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70fd0e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am sajib saha.',\n",
       " 'Working at CIT.',\n",
       " 'As an employee machine Learnig.',\n",
       " 'Do you want to come']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918bb82",
   "metadata": {},
   "source": [
    "# using transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2dce42cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
      "     ------------------------------------ 131.1/131.1 kB 387.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.21.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp39-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp39-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
      "   ---------------------------------------- 8.5/8.5 MB 143.1 kB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.21.1-py3-none-any.whl (346 kB)\n",
      "   -------------------------------------- 346.1/346.1 kB 219.3 kB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp39-none-win_amd64.whl (269 kB)\n",
      "   -------------------------------------- 269.7/269.7 kB 195.3 kB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp39-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 2.2/2.2 MB 314.7 kB/s eta 0:00:00Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "   -------------------------------------- 170.9/170.9 kB 380.8 kB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.2.0\n",
      "    Uninstalling fsspec-2022.2.0:\n",
      "      Successfully uninstalled fsspec-2022.2.0\n",
      "Successfully installed fsspec-2024.2.0 huggingface-hub-0.21.1 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.38.1\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca68cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be7f7af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b39f1af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'I am sajib saha. Working at CIT. As an employee machine Learnig. Do you want to come'\n",
    "token = tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b5f7ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'sa',\n",
       " '##ji',\n",
       " '##b',\n",
       " 'sa',\n",
       " '##ha',\n",
       " '.',\n",
       " 'working',\n",
       " 'at',\n",
       " 'ci',\n",
       " '##t',\n",
       " '.',\n",
       " 'as',\n",
       " 'an',\n",
       " 'employee',\n",
       " 'machine',\n",
       " 'learn',\n",
       " '##ig',\n",
       " '.',\n",
       " 'do',\n",
       " 'you',\n",
       " 'want',\n",
       " 'to',\n",
       " 'come']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad86b31",
   "metadata": {},
   "source": [
    "# Named Entity Tokenization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c363f779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading avaraged_perceptron_tagger: Package\n",
      "[nltk_data]     'avaraged_perceptron_tagger' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('avaraged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe7997ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "159ac66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker') # download the required resource \n",
    "nltk.download('words') # word corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1aea2b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0da61ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'I am Sajib Saha. Working at CIT. As an employee Machine Learnig. Do you want to come join cit'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df763967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d3b4274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sajib Saha', 'CIT', 'Machine Learnig']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(sentence) # tokenize the words\n",
    "pos_tags = pos_tag(tokens) # perform parts of speech tagging\n",
    "ner_tag = ne_chunk(pos_tags) # perform named entity recognition\n",
    "named_token = [] # empty list to store object value \n",
    "\n",
    "for chunk in ner_tag:\n",
    "    if hasattr(chunk, 'label'): # hasattr is (object , attribuate)\n",
    "        named_token.append(' '.join(c[0] for c in chunk ))\n",
    "        \n",
    "print(named_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d393687",
   "metadata": {},
   "source": [
    "# text vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c913a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "69da8873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love Bangladesh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Could you give me an iphone?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello how are you?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to talk you.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           test  class\n",
       "0             I love Bangladesh      1\n",
       "1  Could you give me an iphone?      0\n",
       "2            Hello how are you?      1\n",
       "3           I want to talk you.      1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0266ff",
   "metadata": {},
   "source": [
    "# Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4e5387ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "01c54602",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "11a28759",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_x = cv.fit_transform(data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4e1a8ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x14 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d629ce44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a2f1b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['an',\n",
       " 'are',\n",
       " 'bangladesh',\n",
       " 'could',\n",
       " 'give',\n",
       " 'hello',\n",
       " 'how',\n",
       " 'iphone',\n",
       " 'love',\n",
       " 'me',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'want',\n",
       " 'you']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fed4d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df = pd.DataFrame(cv_x.toarray(), columns = cv.get_feature_names(), index = data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fc6af4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>are</th>\n",
       "      <th>bangladesh</th>\n",
       "      <th>could</th>\n",
       "      <th>give</th>\n",
       "      <th>hello</th>\n",
       "      <th>how</th>\n",
       "      <th>iphone</th>\n",
       "      <th>love</th>\n",
       "      <th>me</th>\n",
       "      <th>talk</th>\n",
       "      <th>to</th>\n",
       "      <th>want</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I love Bangladesh</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Could you give me an iphone?</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hello how are you?</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I want to talk you.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              an  are  bangladesh  could  give  hello  how  \\\n",
       "test                                                                         \n",
       "I love Bangladesh              0    0           1      0     0      0    0   \n",
       "Could you give me an iphone?   1    0           0      1     1      0    0   \n",
       "Hello how are you?             0    1           0      0     0      1    1   \n",
       "I want to talk you.            0    0           0      0     0      0    0   \n",
       "\n",
       "                              iphone  love  me  talk  to  want  you  \n",
       "test                                                                 \n",
       "I love Bangladesh                  0     1   0     0   0     0    0  \n",
       "Could you give me an iphone?       1     0   1     0   0     0    1  \n",
       "Hello how are you?                 0     0   0     0   0     0    1  \n",
       "I want to talk you.                0     0   0     1   1     1    1  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6101922e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cv_df = pd.DataFrame(cv_x.toarray(), columns = cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "69ec21df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>are</th>\n",
       "      <th>bangladesh</th>\n",
       "      <th>could</th>\n",
       "      <th>give</th>\n",
       "      <th>hello</th>\n",
       "      <th>how</th>\n",
       "      <th>iphone</th>\n",
       "      <th>love</th>\n",
       "      <th>me</th>\n",
       "      <th>talk</th>\n",
       "      <th>to</th>\n",
       "      <th>want</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   an  are  bangladesh  could  give  hello  how  iphone  love  me  talk  to  \\\n",
       "0   0    0           1      0     0      0    0       0     1   0     0   0   \n",
       "1   1    0           0      1     1      0    0       1     0   1     0   0   \n",
       "2   0    1           0      0     0      1    1       0     0   0     0   0   \n",
       "3   0    0           0      0     0      0    0       0     0   0     1   1   \n",
       "\n",
       "   want  you  \n",
       "0     0    0  \n",
       "1     0    1  \n",
       "2     0    1  \n",
       "3     1    1  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262527c",
   "metadata": {},
   "source": [
    "# TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "36600b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f228e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_y = tf.fit_transform (data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d66d6862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x14 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fc811421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.70710678, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.70710678, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.43003652, 0.        , 0.        , 0.43003652, 0.43003652,\n",
       "        0.        , 0.        , 0.43003652, 0.        , 0.43003652,\n",
       "        0.        , 0.        , 0.        , 0.27448674],\n",
       "       [0.        , 0.5417361 , 0.        , 0.        , 0.        ,\n",
       "        0.5417361 , 0.5417361 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.34578314],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5417361 , 0.5417361 , 0.5417361 , 0.34578314]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_y.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "66d36bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['an',\n",
       " 'are',\n",
       " 'bangladesh',\n",
       " 'could',\n",
       " 'give',\n",
       " 'hello',\n",
       " 'how',\n",
       " 'iphone',\n",
       " 'love',\n",
       " 'me',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'want',\n",
       " 'you']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d2b1ba01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>are</th>\n",
       "      <th>bangladesh</th>\n",
       "      <th>could</th>\n",
       "      <th>give</th>\n",
       "      <th>hello</th>\n",
       "      <th>how</th>\n",
       "      <th>iphone</th>\n",
       "      <th>love</th>\n",
       "      <th>me</th>\n",
       "      <th>talk</th>\n",
       "      <th>to</th>\n",
       "      <th>want</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I love Bangladesh</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Could you give me an iphone?</th>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hello how are you?</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I want to talk you.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.345783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    an       are  bangladesh     could  \\\n",
       "test                                                                     \n",
       "I love Bangladesh             0.000000  0.000000    0.707107  0.000000   \n",
       "Could you give me an iphone?  0.430037  0.000000    0.000000  0.430037   \n",
       "Hello how are you?            0.000000  0.541736    0.000000  0.000000   \n",
       "I want to talk you.           0.000000  0.000000    0.000000  0.000000   \n",
       "\n",
       "                                  give     hello       how    iphone  \\\n",
       "test                                                                   \n",
       "I love Bangladesh             0.000000  0.000000  0.000000  0.000000   \n",
       "Could you give me an iphone?  0.430037  0.000000  0.000000  0.430037   \n",
       "Hello how are you?            0.000000  0.541736  0.541736  0.000000   \n",
       "I want to talk you.           0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                                  love        me      talk        to  \\\n",
       "test                                                                   \n",
       "I love Bangladesh             0.707107  0.000000  0.000000  0.000000   \n",
       "Could you give me an iphone?  0.000000  0.430037  0.000000  0.000000   \n",
       "Hello how are you?            0.000000  0.000000  0.000000  0.000000   \n",
       "I want to talk you.           0.000000  0.000000  0.541736  0.541736   \n",
       "\n",
       "                                  want       you  \n",
       "test                                              \n",
       "I love Bangladesh             0.000000  0.000000  \n",
       "Could you give me an iphone?  0.000000  0.274487  \n",
       "Hello how are you?            0.000000  0.345783  \n",
       "I want to talk you.           0.541736  0.345783  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df = pd.DataFrame(tf_y.toarray(), columns=tf.get_feature_names(),  index = data['test'])\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc33372c",
   "metadata": {},
   "source": [
    "# Wrod2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fec38f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading gensim-4.3.2-cp311-cp311-win_amd64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
      "Downloading gensim-4.3.2-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 24.0/24.0 MB 339.9 kB/s eta 0:00:00\n",
      "Downloading smart_open-7.0.1-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 60.8/60.8 kB 815.3 kB/s eta 0:00:00\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-7.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "211c828e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'love', 'Bangladesh'],\n",
       " ['Could', 'you', 'give', 'me', 'an', 'iphone', '?'],\n",
       " ['Hello', 'how', 'are', 'you', '?'],\n",
       " ['I', 'want', 'to', 'talk', 'you', '.']]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "text_vector = [nltk.word_tokenize(test) for test in data['test']]\n",
    "text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a731a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(text_vector, min_count= 1) # lowest count 5 in number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "abbc845e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('an', 0.17826786637306213),\n",
       " ('I', 0.16072483360767365),\n",
       " ('give', 0.10560770332813263),\n",
       " ('how', 0.09215974807739258),\n",
       " ('iphone', 0.048910051584243774),\n",
       " ('are', 0.02700837142765522),\n",
       " ('Could', 0.007729300297796726),\n",
       " ('you', -0.03771638125181198),\n",
       " ('.', -0.04552280902862549),\n",
       " ('talk', -0.0464920699596405)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('want')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8855e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
